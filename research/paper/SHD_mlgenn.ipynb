{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "\n",
    "from ml_genn import Connection, Network, Population\n",
    "from ml_genn.callbacks import Checkpoint, SpikeRecorder, VarRecorder\n",
    "from ml_genn.compilers import EventPropCompiler, InferenceCompiler\n",
    "from ml_genn.connectivity import Dense,FixedProbability\n",
    "from ml_genn.initializers import Normal\n",
    "from ml_genn.neurons import LeakyIntegrate, LeakyIntegrateFire, SpikeInput\n",
    "from ml_genn.optimisers import Adam\n",
    "from ml_genn.serialisers import Numpy\n",
    "from ml_genn.synapses import Exponential\n",
    "from tonic.datasets import SHD\n",
    "from tonic import transforms\n",
    "\n",
    "from time import perf_counter\n",
    "from ml_genn.utils.data import (calc_latest_spike_time, calc_max_spikes,\n",
    "                                preprocess_tonic_spikes)\n",
    "\n",
    "from ml_genn.compilers.event_prop_compiler import default_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max spikes train 14917, latest spike time train 1369.0\n",
      "Max spikes test 16257, latest spike time test 1169.0\n"
     ]
    }
   ],
   "source": [
    "sample_T = 1000 #64\n",
    "shd_channels = 700\n",
    "net_channels = 700 #128\n",
    "# note that mlGeNN works in units of ms\n",
    "net_dt = 1000/sample_T\n",
    "\n",
    "NUM_HIDDEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "EXAMPLE_TIME = 20.0\n",
    "KERNEL_PROFILING = True\n",
    "\n",
    "\n",
    "transform = transforms.Downsample(\n",
    "        time_factor=1,\n",
    "        spatial_factor=net_channels / shd_channels\n",
    "    )\n",
    "\n",
    "# Get SHD dataset\n",
    "dataset= {}\n",
    "dataset[\"train\"] = SHD(save_to='./data', train=True, transform=transform)\n",
    "dataset[\"test\"] = SHD(save_to='./data', train=False, transform=transform)\n",
    "\n",
    "# Preprocess\n",
    "spikes= {}\n",
    "labels= {}\n",
    "for which in [\"train\",\"test\"]:\n",
    "    spikes[which] = []\n",
    "    labels[which] = []\n",
    "    for i in range(len(dataset[which])):\n",
    "        events, label = dataset[which][i]\n",
    "        spikes[which].append(preprocess_tonic_spikes(events, dataset[which].ordering,\n",
    "                                              dataset[which].sensor_size, dt=net_dt, histogram_thresh=1))\n",
    "        labels[which].append(label)\n",
    "\n",
    "# Determine max spikes and latest spike time\n",
    "max_spikes = {}\n",
    "latest_spike_time = {}\n",
    "for which in [\"train\",\"test\"]:\n",
    "    max_spikes[which] = calc_max_spikes(spikes[which])\n",
    "    latest_spike_time[which] = calc_latest_spike_time(spikes[which])\n",
    "\n",
    "for which in [\"train\",\"test\"]:\n",
    "    print(f\"Max spikes {which} {max_spikes[which]}, latest spike time {which} {latest_spike_time[which]}\")\n",
    "\n",
    "# Get number of input and output neurons from dataset \n",
    "# and round up outputs to power-of-two\n",
    "# these are the same for train and test\n",
    "num_input = int(np.prod(dataset[\"train\"].sensor_size))\n",
    "num_output = len(dataset[\"train\"].classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8156\n",
      "2264\n"
     ]
    }
   ],
   "source": [
    "print(len(spikes[\"train\"]))\n",
    "print(len(spikes[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./EventPropCompiler_CODE/customUpdateCUDAOptim.cc(529): warning: variable \"batchOffset\" was declared but never referenced\n",
      "\n",
      "./EventPropCompiler_CODE/neuronUpdateCUDAOptim.cc(282): warning: variable \"lYTrue\" was declared but never referenced\n",
      "\n",
      "./EventPropCompiler_CODE/neuronUpdateCUDAOptim.cc(282): warning: variable \"lYTrue\" was declared but never referenced\n",
      "\n",
      "./EventPropCompiler_CODE/customUpdateCUDAOptim.cc(529): warning: variable \"batchOffset\" was declared but never referenced\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/its/home/tn41/spyx/research/paper/EventPropCompiler_CODE'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neuronUpdate.cc(282): warning: variable \"lYTrue\" was declared but never referenced\n",
      "\n",
      "customUpdate.cc(529): warning: variable \"batchOffset\" was declared but never referenced\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Leaving directory '/its/home/tn41/spyx/research/paper/EventPropCompiler_CODE'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015316009521484375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 255,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550f373ff2d741f8a05fd839fabed1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 85.42177538008828%\n",
      "Time = 1306.8711126269773s\n",
      "Neuron update time = 281.534813520676\n",
      "Presynaptic update time = 446.8579314324223\n",
      "Gradient batch reduce time = 7.168805803238494\n",
      "Gradient learn time = 0.47280346669059126\n",
      "Reset time = 0.24571251416058992\n",
      "Softmax1 time = 0.15397123452222825\n",
      "Softmax2 time = 0.14953955355187823\n",
      "Softmax3 time = 0.14467980824406626\n"
     ]
    }
   ],
   "source": [
    "serialiser = Numpy(\"shd_checkpoints\")\n",
    "network = Network(default_params)\n",
    "with network:\n",
    "    # Populations\n",
    "    input = Population(SpikeInput(max_spikes=BATCH_SIZE * max_spikes[\"train\"]),\n",
    "                       num_input)\n",
    "    hidden = Population(LeakyIntegrateFire(v_thresh=1.0, tau_mem=20.0,\n",
    "                                           tau_refrac=None),\n",
    "                        NUM_HIDDEN)\n",
    "    output = Population(LeakyIntegrate(tau_mem=20.0, readout=\"avg_var_exp_weight\"),\n",
    "                        num_output)\n",
    "\n",
    "    # Connections\n",
    "    Connection(input, hidden, Dense(Normal(mean=0.03, sd=0.01)),\n",
    "               Exponential(5.0))\n",
    "    Connection(hidden, hidden, Dense(Normal(mean=0.0, sd=0.02)),\n",
    "               Exponential(5.0))\n",
    "    Connection(hidden, output, Dense(Normal(mean=0.0, sd=0.03)),\n",
    "               Exponential(5.0))\n",
    "\n",
    "max_example_timesteps = int(np.ceil(latest_spike_time[\"train\"] / net_dt))\n",
    "\n",
    "compiler = EventPropCompiler(example_timesteps=max_example_timesteps,\n",
    "                             losses=\"sparse_categorical_crossentropy\",\n",
    "                             reg_lambda_upper=4e-09, reg_lambda_lower=4e-09, \n",
    "                             reg_nu_upper=14, max_spikes=1500, \n",
    "                             optimiser=Adam(0.001), batch_size=BATCH_SIZE, \n",
    "                             kernel_profiling=KERNEL_PROFILING)\n",
    "compiled_net = compiler.compile(network)\n",
    "\n",
    "with compiled_net:\n",
    "    # Evaluate model on numpy dataset\n",
    "    start_time = perf_counter()\n",
    "    callbacks = [\"batch_progress_bar\", Checkpoint(serialiser)]\n",
    "    metrics, _  = compiled_net.train({input: spikes[\"train\"]},\n",
    "                                     {output: labels[\"train\"]},\n",
    "                                     num_epochs=NUM_EPOCHS, shuffle=True,\n",
    "                                     callbacks=callbacks)\n",
    "    compiled_net.save_connectivity((NUM_EPOCHS - 1,), serialiser)\n",
    "\n",
    "    end_time = perf_counter()\n",
    "    print(f\"Accuracy = {100 * metrics[output].result}%\")\n",
    "    print(f\"Time = {end_time - start_time}s\")\n",
    "\n",
    "    if KERNEL_PROFILING:\n",
    "        print(f\"Neuron update time = {compiled_net.genn_model.neuron_update_time}\")\n",
    "        print(f\"Presynaptic update time = {compiled_net.genn_model.presynaptic_update_time}\")\n",
    "        print(f\"Gradient batch reduce time = {compiled_net.genn_model.get_custom_update_time('GradientBatchReduce')}\")\n",
    "        print(f\"Gradient learn time = {compiled_net.genn_model.get_custom_update_time('GradientLearn')}\")\n",
    "        print(f\"Reset time = {compiled_net.genn_model.get_custom_update_time('Reset')}\")\n",
    "        print(f\"Softmax1 time = {compiled_net.genn_model.get_custom_update_time('BatchSoftmax1')}\")\n",
    "        print(f\"Softmax2 time = {compiled_net.genn_model.get_custom_update_time('BatchSoftmax2')}\")\n",
    "        print(f\"Softmax3 time = {compiled_net.genn_model.get_custom_update_time('BatchSoftmax3')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./InferenceCompiler_CODE/synapseUpdateCUDAOptim.cc(49): warning: variable \"synBatchOffset\" was declared but never referenced\n",
      "\n",
      "./InferenceCompiler_CODE/synapseUpdateCUDAOptim.cc(49): warning: variable \"synBatchOffset\" was declared but never referenced\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/its/home/tn41/spyx/research/paper/InferenceCompiler_CODE'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "synapseUpdate.cc(49): warning: variable \"synBatchOffset\" was declared but never referenced\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Leaving directory '/its/home/tn41/spyx/research/paper/InferenceCompiler_CODE'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016054868698120117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 71,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2307dbe7d5c41278bccb306bc4acbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 68.28621908127208%\n",
      "Time = 1.2334860870032571s\n"
     ]
    }
   ],
   "source": [
    "# Load network state from final checkpoint\n",
    "network.load((NUM_EPOCHS - 1,), serialiser)\n",
    "\n",
    "compiler = InferenceCompiler(evaluate_timesteps=max_example_timesteps,\n",
    "                            reset_in_syn_between_batches=True,\n",
    "                            batch_size=BATCH_SIZE)\n",
    "compiled_net = compiler.compile(network)\n",
    "\n",
    "with compiled_net:\n",
    "    # Evaluate model on numpy dataset\n",
    "    start_time = perf_counter()\n",
    "    metrics, _  = compiled_net.evaluate({input: spikes[\"test\"]},\n",
    "                                        {output: labels[\"test\"]})\n",
    "    end_time = perf_counter()\n",
    "    print(f\"Accuracy = {100 * metrics[output].result}%\")\n",
    "    print(f\"Time = {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
