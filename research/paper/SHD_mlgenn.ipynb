{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ml_genn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8218/183279533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mml_genn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mml_genn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVarRecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mml_genn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompilers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEPropCompiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInferenceCompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ml_genn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "\n",
    "from ml_genn import Connection, Network, Population\n",
    "from ml_genn.callbacks import Checkpoint, SpikeRecorder, VarRecorder\n",
    "from ml_genn.compilers import EventPropCompiler, InferenceCompiler\n",
    "from ml_genn.connectivity import Dense,FixedProbability\n",
    "from ml_genn.initializers import Normal\n",
    "from ml_genn.neurons import LeakyIntegrate, LeakyIntegrateFire, SpikeInput\n",
    "from ml_genn.optimisers import Adam\n",
    "from ml_genn.serialisers import Numpy\n",
    "from ml_genn.synapses import Exponential\n",
    "from tonic.datasets import SHD\n",
    "\n",
    "from time import perf_counter\n",
    "from ml_genn.utils.data import (calc_latest_spike_time, calc_max_spikes,\n",
    "                                preprocess_tonic_spikes)\n",
    "\n",
    "from ml_genn.compilers.event_prop_compiler import default_params\n",
    "\n",
    "NUM_HIDDEN = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 300\n",
    "EXAMPLE_TIME = 20.0\n",
    "DT = 1.0\n",
    "TRAIN = True\n",
    "KERNEL_PROFILING = True\n",
    "\n",
    "# Get SHD dataset\n",
    "dataset = SHD(save_to='../data', train=TRAIN)\n",
    "\n",
    "# Preprocess\n",
    "spikes = []\n",
    "labels = []\n",
    "for i in range(len(dataset)):\n",
    "    events, label = dataset[i]\n",
    "    spikes.append(preprocess_tonic_spikes(events, dataset.ordering,\n",
    "                                          dataset.sensor_size))\n",
    "    labels.append(label)\n",
    "\n",
    "# Determine max spikes and latest spike time\n",
    "max_spikes = calc_max_spikes(spikes)\n",
    "latest_spike_time = calc_latest_spike_time(spikes)\n",
    "print(f\"Max spikes {max_spikes}, latest spike time {latest_spike_time}\")\n",
    "\n",
    "# Get number of input and output neurons from dataset \n",
    "# and round up outputs to power-of-two\n",
    "num_input = int(np.prod(dataset.sensor_size))\n",
    "num_output = len(dataset.classes)\n",
    "\n",
    "serialiser = Numpy(\"shd_checkpoints\")\n",
    "network = Network(default_params)\n",
    "with network:\n",
    "    # Populations\n",
    "    input = Population(SpikeInput(max_spikes=BATCH_SIZE * max_spikes),\n",
    "                       num_input)\n",
    "    hidden = Population(LeakyIntegrateFire(v_thresh=1.0, tau_mem=20.0,\n",
    "                                           tau_refrac=None),\n",
    "                        NUM_HIDDEN)\n",
    "    output = Population(LeakyIntegrate(tau_mem=20.0, readout=\"avg_var_exp_weight\"),\n",
    "                        num_output)\n",
    "\n",
    "    # Connections\n",
    "    Connection(input, hidden, Dense(Normal(mean=0.03, sd=0.01)),\n",
    "               Exponential(5.0))\n",
    "    Connection(hidden, hidden, Dense(Normal(mean=0.0, sd=0.02)),\n",
    "               Exponential(5.0))\n",
    "    Connection(hidden, output, Dense(Normal(mean=0.0, sd=0.03)),\n",
    "               Exponential(5.0))\n",
    "\n",
    "max_example_timesteps = int(np.ceil(latest_spike_time / DT))\n",
    "if TRAIN:\n",
    "    compiler = EventPropCompiler(example_timesteps=max_example_timesteps,\n",
    "                                 losses=\"sparse_categorical_crossentropy\",\n",
    "                                 reg_lambda_upper=4e-09, reg_lambda_lower=4e-09, \n",
    "                                 reg_nu_upper=14, max_spikes=1500, \n",
    "                                 optimiser=Adam(0.001), batch_size=BATCH_SIZE, \n",
    "                                 kernel_profiling=KERNEL_PROFILING)\n",
    "    compiled_net = compiler.compile(network)\n",
    "\n",
    "    with compiled_net:\n",
    "        # Evaluate model on numpy dataset\n",
    "        start_time = perf_counter()\n",
    "        callbacks = [\"batch_progress_bar\", Checkpoint(serialiser)]\n",
    "        metrics, _  = compiled_net.train({input: spikes},\n",
    "                                         {output: labels},\n",
    "                                         num_epochs=NUM_EPOCHS, shuffle=True,\n",
    "                                         callbacks=callbacks)\n",
    "        compiled_net.save_connectivity((NUM_EPOCHS - 1,), serialiser)\n",
    "\n",
    "        end_time = perf_counter()\n",
    "        print(f\"Accuracy = {100 * metrics[output].result}%\")\n",
    "        print(f\"Time = {end_time - start_time}s\")\n",
    "\n",
    "        if KERNEL_PROFILING:\n",
    "            print(f\"Neuron update time = {compiled_net.genn_model.neuron_update_time}\")\n",
    "            print(f\"Presynaptic update time = {compiled_net.genn_model.presynaptic_update_time}\")\n",
    "            print(f\"Gradient batch reduce time = {compiled_net.genn_model.get_custom_update_time('GradientBatchReduce')}\")\n",
    "            print(f\"Gradient learn time = {compiled_net.genn_model.get_custom_update_time('GradientLearn')}\")\n",
    "            print(f\"Reset time = {compiled_net.genn_model.get_custom_update_time('Reset')}\")\n",
    "            print(f\"Softmax1 time = {compiled_net.genn_model.get_custom_update_time('BatchSoftmax1')}\")\n",
    "            print(f\"Softmax2 time = {compiled_net.genn_model.get_custom_update_time('BatchSoftmax2')}\")\n",
    "            print(f\"Softmax3 time = {compiled_net.genn_model.get_custom_update_time('BatchSoftmax3')}\")\n",
    "else:\n",
    "    # Load network state from final checkpoint\n",
    "    network.load((NUM_EPOCHS - 1,), serialiser)\n",
    "\n",
    "    compiler = InferenceCompiler(evaluate_timesteps=max_example_timesteps,\n",
    "                                 reset_in_syn_between_batches=True,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "    compiled_net = compiler.compile(network)\n",
    "\n",
    "    with compiled_net:\n",
    "        # Evaluate model on numpy dataset\n",
    "        start_time = perf_counter()\n",
    "        metrics, _  = compiled_net.evaluate({input: spikes},\n",
    "                                            {output: labels})\n",
    "        end_time = perf_counter()\n",
    "        print(f\"Accuracy = {100 * metrics[output].result}%\")\n",
    "        print(f\"Time = {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
