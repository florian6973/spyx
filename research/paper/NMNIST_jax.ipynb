{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6793a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".80\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import spyx\n",
    "import haiku as hk\n",
    "import optax\n",
    "from jax_tqdm import scan_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30d5e6",
   "metadata": {},
   "source": [
    "### SHD Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab70fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tonic\n",
    "from tonic import datasets, transforms\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple(\"State\", \"obs labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a2e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Denoise removes isolated, one-off events\n",
    "# time_window\n",
    "frame_transform = transforms.Compose([\n",
    "                                      transforms.ToFrame(sensor_size=sensor_size, \n",
    "                                                         n_time_bins=64),\n",
    "                                      lambda x: np.packbits(x, axis=0)\n",
    "                                     ])\n",
    "\n",
    "train_dataset = tonic.datasets.NMNIST(save_to='./tmp/data', first_saccade_only=True, transform=frame_transform, train=True)\n",
    "test_dataset = tonic.datasets.NMNIST(save_to='./tmp/data', first_saccade_only=True, transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf6be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(DataLoader(train_dataset, batch_size=len(train_dataset),\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=True), drop_last=True, shuffle=False))\n",
    "        \n",
    "x_train, y_train = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2544a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = iter(DataLoader(test_dataset, batch_size=len(test_dataset),\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=True), drop_last=True, shuffle=False))\n",
    "        \n",
    "x_test, y_test = next(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670ea0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = jnp.array(x_train, dtype=jnp.uint8)\n",
    "y_train = jnp.array(y_train, dtype=jnp.uint8)\n",
    "\n",
    "x_test = jnp.array(x_test, dtype=jnp.uint8)\n",
    "y_test = jnp.array(y_test, dtype=jnp.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7135a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset, shuffle_rng):\n",
    "    x, y = dataset\n",
    "\n",
    "    cutoff = y.shape[0] % batch_size\n",
    "\n",
    "    obs = jax.random.permutation(shuffle_rng, x, axis=0)[:-cutoff]\n",
    "    labels = jax.random.permutation(shuffle_rng, y, axis=0)[:-cutoff]\n",
    "\n",
    "    obs = jnp.reshape(obs, (-1, batch_size) + obs.shape[1:])\n",
    "    labels = jnp.reshape(labels, (-1, batch_size)) # should make batch size a global\n",
    "\n",
    "    return State(obs=obs, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f415eb8",
   "metadata": {},
   "source": [
    "### Spyx SHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddc797e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shd_snn(x): \n",
    "        \n",
    "    core = hk.DeepRNN([\n",
    "        hk.Conv2d(2, 12, 5),\n",
    "        spyx.nn.LIF((64,), activation=spyx.axn.Axon(spyx.axn.arctan())),\n",
    "        hk.MaxPool2d(2),\n",
    "        hk.Conv2d(12, 32, 5),\n",
    "        spyx.nn.LIF((64,), activation=spyx.axn.Axon(spyx.axn.arctan())),\n",
    "        hk.MaxPool2d(),\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(20, with_bias=False),\n",
    "        spyx.nn.LIF((20,), activation=spyx.axn.Axon(spyx.axn.arctan()))\n",
    "    ])\n",
    "    \n",
    "    # static unroll for maximum performance\n",
    "    spikes, V = hk.dynamic_unroll(core, x, core.initial_state(x.shape[0]), time_major=False, unroll=32)\n",
    "    \n",
    "    return spikes, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344ed98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "sample_x, sample_y = shuffle((x_train,y_train),key)\n",
    "SNN = hk.without_apply_rng(hk.transform(shd_snn))\n",
    "params = SNN.init(rng=key, x=jnp.float32(sample_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f7422a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(SNN, params, dataset, epochs=300):\n",
    "        \n",
    "    opt = optax.adam(learning_rate=5e-4)\n",
    "    \n",
    "    # create and initialize the optimizer\n",
    "    opt_state = opt.init(params)\n",
    "    grad_params = params\n",
    "        \n",
    "    # define and compile our eval function that computes the loss for our SNN\n",
    "    @jax.jit\n",
    "    def net_eval(weights, events, targets):\n",
    "        readout = SNN.apply(weights, events)\n",
    "        traces, V_f = readout\n",
    "        return spyx.fn.integral_crossentropy(traces, targets) # smoothing needs to be more explicit in docs...\n",
    "        \n",
    "    # Use JAX to create a function that calculates the loss and the gradient!\n",
    "    surrogate_grad = jax.value_and_grad(net_eval) \n",
    "        \n",
    "    rng = jax.random.PRNGKey(0)        \n",
    "    \n",
    "    # compile the meat of our training loop for speed\n",
    "    @jax.jit\n",
    "    def train_step(state, data):\n",
    "        grad_params, opt_state = state\n",
    "        events, targets = data # fix this\n",
    "        events = jnp.unpackbits(events, axis=1) # decompress temporal axis\n",
    "        # compute loss and gradient\n",
    "        loss, grads = surrogate_grad(grad_params, events, targets)\n",
    "        # generate updates based on the gradients and optimizer\n",
    "        updates, opt_state = opt.update(grads, opt_state, grad_params)\n",
    "        # return the updated parameters\n",
    "        new_state = [optax.apply_updates(grad_params, updates), opt_state]\n",
    "        return new_state, loss\n",
    "    \n",
    "    \n",
    "    # Here's the start of our training loop!\n",
    "    @scan_tqdm(epochs)\n",
    "    def epoch(epoch_state, epoch_num):\n",
    "        curr_params, curr_opt_state = epoch_state\n",
    "\n",
    "        shuffle_rng = jax.random.fold_in(rng, epoch_num)\n",
    "        train_data = shuffle(dataset, shuffle_rng)\n",
    "        \n",
    "        # train epoch\n",
    "        end_state, train_loss = jax.lax.scan(\n",
    "            train_step,# func\n",
    "            [curr_params, curr_opt_state],# init\n",
    "            train_data,# xs\n",
    "            train_data.obs.shape[0]# len\n",
    "        )\n",
    "                    \n",
    "        return end_state, jnp.mean(train_loss)\n",
    "    # end epoch\n",
    "    \n",
    "    # epoch loop\n",
    "    final_state, metrics = jax.lax.scan(\n",
    "        epoch,\n",
    "        [grad_params, opt_state], # metric arrays\n",
    "        jnp.arange(epochs), # \n",
    "        epochs # len of loop\n",
    "    )\n",
    "    \n",
    "    final_params, _ = final_state\n",
    "    \n",
    "                \n",
    "    # return our final, optimized network.       \n",
    "    return final_params, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gd(SNN, params, dataset):\n",
    "\n",
    "    @jax.jit\n",
    "    def test_step(params, data):\n",
    "        events, targets = data\n",
    "        events = jnp.unpackbits(events, axis=1)\n",
    "        readout = SNN.apply(params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.fn.integral_accuracy(traces, targets)\n",
    "        loss = spyx.fn.integral_crossentropy(traces, targets)\n",
    "        return params, [acc, loss, pred, targets]\n",
    "    \n",
    "    test_data = shuffle(dataset, jax.random.PRNGKey(0))\n",
    "    \n",
    "    _, test_metrics = jax.lax.scan(\n",
    "            test_step,# func\n",
    "            params,# init\n",
    "            test_data,# xs\n",
    "            test_data.obs.shape[0]# len\n",
    "    )\n",
    "    \n",
    "    acc = jnp.mean(test_metrics[0])\n",
    "    loss = jnp.mean(test_metrics[1])\n",
    "    preds = jnp.array(test_metrics[2]).flatten()\n",
    "    tgts = jnp.array(test_metrics[3]).flatten()\n",
    "    return acc, loss, preds, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd08a737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da322de3463a430da0ad3fac92d77297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_params, metrics = gd(SNN, params, (x_train,y_train), epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a3a0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([3.3948634, 3.0212855, 3.03532  , 3.023731 , 2.9759493, 2.8594928,\n",
       "       2.7836218, 2.7019773, 2.6405947, 2.5995522, 2.5443957, 2.5140488,\n",
       "       2.47452  , 2.4482877, 2.4146333, 2.376006 , 2.3507137, 2.3295076,\n",
       "       2.3165836, 2.2969234, 2.2816613, 2.268716 , 2.2536645, 2.227889 ,\n",
       "       2.2070014, 2.1970618, 2.1781995, 2.160053 , 2.135837 , 2.123934 ,\n",
       "       2.1015546, 2.0945477, 2.07775  , 2.0695205, 2.0600803, 2.0559309,\n",
       "       2.0453296, 2.0422237, 2.031036 , 2.0284038, 2.018376 , 2.0102658,\n",
       "       2.006234 , 2.008062 , 1.9893986, 1.9828401, 1.9807476, 1.982857 ,\n",
       "       1.9694746, 1.9630796, 1.967521 , 1.9541516, 1.9514204, 1.9487829,\n",
       "       1.941763 , 1.9421433, 1.9427478, 1.9334282, 1.9261316, 1.9253156,\n",
       "       1.9206196, 1.9161831, 1.9150175, 1.9136933, 1.9085118, 1.9031168,\n",
       "       1.9061825, 1.8991681, 1.8885156, 1.8890601, 1.8875759, 1.8813595,\n",
       "       1.8773499, 1.8780856, 1.8788662, 1.8749352, 1.8716233, 1.868697 ,\n",
       "       1.8597573, 1.8643638, 1.8605853, 1.8526983, 1.8532511, 1.8483022,\n",
       "       1.8444868, 1.8454276, 1.8424217, 1.8474938, 1.8406624, 1.8418344,\n",
       "       1.8355602, 1.8339511, 1.834626 , 1.8278894, 1.8235607, 1.8256484,\n",
       "       1.8293625, 1.8245518, 1.8234607, 1.8232243, 1.8209864, 1.8239832,\n",
       "       1.815807 , 1.8122487, 1.8119595, 1.8072643, 1.8080082, 1.804586 ,\n",
       "       1.8032104, 1.8057883, 1.8046552, 1.8026264, 1.8026973, 1.7985926,\n",
       "       1.7959546, 1.7935311, 1.7908412, 1.792127 , 1.7886959, 1.7872984,\n",
       "       1.7867872, 1.7872301, 1.7851027, 1.7817367, 1.7813106, 1.7790653,\n",
       "       1.7784239, 1.7754054, 1.7816126, 1.7717433, 1.7747308, 1.7729328,\n",
       "       1.7701422, 1.772069 , 1.7698389, 1.7638968, 1.7654232, 1.7664055,\n",
       "       1.7666305, 1.7652955, 1.7641181, 1.7627165, 1.7581675, 1.759827 ,\n",
       "       1.7620103, 1.7576758, 1.7542738, 1.751576 , 1.7526553, 1.7596306,\n",
       "       1.7558335, 1.7533858, 1.7476264, 1.7497956, 1.7495158, 1.744324 ,\n",
       "       1.7458339, 1.7535079, 1.7382423, 1.7454878, 1.7405835, 1.7409315,\n",
       "       1.7407271, 1.7359477, 1.741633 , 1.735945 , 1.734313 , 1.7358464,\n",
       "       1.7300401, 1.7300757, 1.7312967, 1.729202 , 1.7291758, 1.7275515,\n",
       "       1.7253152, 1.7234356, 1.7234403, 1.7179579, 1.7241774, 1.7194422,\n",
       "       1.718521 , 1.7216407, 1.7184855, 1.7245991, 1.7171528, 1.7150829,\n",
       "       1.7155246, 1.7182837, 1.7157811, 1.7144774, 1.711765 , 1.7106589,\n",
       "       1.7101438, 1.7126883, 1.7092817, 1.7058817, 1.7048098, 1.7050645,\n",
       "       1.7030286, 1.7059922, 1.7067958, 1.7036369, 1.7011381, 1.7002727,\n",
       "       1.6996077, 1.6940997, 1.6982701, 1.7013663, 1.7003058, 1.7007226,\n",
       "       1.6973124, 1.6995622, 1.6961218, 1.6930003, 1.6928076, 1.6935388,\n",
       "       1.6914159, 1.6895673, 1.692746 , 1.6886735, 1.6894848, 1.6899284,\n",
       "       1.6912258, 1.6938003, 1.690336 , 1.6837883, 1.6873047, 1.6867769,\n",
       "       1.6828458, 1.6824187, 1.6807439, 1.6812357, 1.6828345, 1.6802331,\n",
       "       1.6796457, 1.6790017, 1.6766762, 1.681798 , 1.6805255, 1.6790162,\n",
       "       1.6778432, 1.6759299, 1.6725135, 1.6746304, 1.6747377, 1.6720122,\n",
       "       1.6750273, 1.6738993, 1.6721056, 1.6683757, 1.6709231, 1.6727896,\n",
       "       1.670916 , 1.6699411, 1.6701039, 1.6674844, 1.6658839, 1.6644901,\n",
       "       1.6684775, 1.6665444, 1.6704246, 1.6692752, 1.6656436, 1.6637527,\n",
       "       1.6709739, 1.6632056, 1.6653951, 1.6626922, 1.6637043, 1.6585082,\n",
       "       1.6579572, 1.6572129, 1.6609129, 1.6581244, 1.6557629, 1.6629175,\n",
       "       1.6590381, 1.6549591, 1.6571958, 1.653735 , 1.6543812, 1.6543992,\n",
       "       1.6553073, 1.6571918, 1.6528995, 1.65954  , 1.6550888, 1.6574918,\n",
       "       1.6506195, 1.6525413, 1.6491196, 1.651813 , 1.6515516, 1.64964  ,\n",
       "       1.6500335, 1.6479303, 1.6473178, 1.6473141, 1.6462982, 1.6487588],      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8553879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74560547 Loss: 1.9869497\n"
     ]
    }
   ],
   "source": [
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, (x_test,y_test))\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
