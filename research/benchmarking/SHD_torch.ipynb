{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import snntorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_per_process_memory_fraction(0.85, device=0)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tonic\n",
    "from tonic import datasets, transforms\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple(\"State\", \"obs labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SHD2Raster():\n",
    "    \"\"\" \n",
    "    Tool for rastering SHD samples into frames. Packs bits along the temporal axis for memory efficiency. This means\n",
    "        that the used will have to apply jnp.unpackbits(events, axis=<time axis>) prior to feeding the data to the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_dim, sample_T = 100):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.sample_T = sample_T\n",
    "        \n",
    "    def __call__(self, events):\n",
    "        # tensor has dimensions (time_steps, encoding_dim)\n",
    "        tensor = np.zeros((events[\"t\"].max()+1, self.encoding_dim), dtype=int)\n",
    "        np.add.at(tensor, (events[\"t\"], events[\"x\"]), 1)\n",
    "        #return tensor[:self.sample_T,:]\n",
    "        tensor = tensor[:self.sample_T,:]\n",
    "        tensor = np.minimum(tensor, 1)\n",
    "        #tensor = np.packbits(tensor, axis=0) pytorch does not have an unpack feature.\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_T = 64\n",
    "shd_timestep = 1e-6\n",
    "shd_channels = 700\n",
    "net_channels = 128\n",
    "net_dt = 1/sample_T\n",
    "batch_size = 256\n",
    "\n",
    "obs_shape = tuple([net_channels,])\n",
    "act_shape = tuple([20,])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Downsample(\n",
    "        time_factor=shd_timestep / net_dt,\n",
    "        spatial_factor=net_channels / shd_channels\n",
    "    ),\n",
    "    _SHD2Raster(net_channels, sample_T=sample_T)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.SHD(\"./data\", train=True, transform=transform)\n",
    "test_dataset = datasets.SHD(\"./data\", train=False, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(DataLoader(train_dataset, batch_size=len(train_dataset),\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=True), drop_last=True, shuffle=False))\n",
    "        \n",
    "x_train, y_train = next(train_dl)\n",
    "x_train, y_train = x_train.to(torch.uint8), y_train.to(torch.uint8)\n",
    "x_train, y_train = x_train.to(device), y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset):\n",
    "    x, y = dataset\n",
    "\n",
    "    cutoff = y.shape[0] % batch_size\n",
    "\n",
    "    indices = torch.randperm(y.shape[0])[:-cutoff]\n",
    "    obs, labels = x[indices], y[indices]\n",
    "\n",
    "\n",
    "    obs = torch.reshape(obs, (-1, batch_size) + obs.shape[1:])\n",
    "    labels = torch.reshape(labels, (-1, batch_size)) # should make batch size a global\n",
    "\n",
    "    return State(obs=obs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = iter(DataLoader(test_dataset, batch_size=len(test_dataset),\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=True), drop_last=True, shuffle=False))\n",
    "        \n",
    "x_test, y_test = next(test_dl)\n",
    "x_test, y_test = x_test.to(torch.uint8), y_test.to(torch.uint8)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "x_test, y_test = shuffle((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "beta = 0.8\n",
    "# Define Network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = torch.nn.Linear(128, num_hidden)\n",
    "        self.lif1 = snntorch.Leaky(beta=beta)\n",
    "        self.fc2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.lif2 = snntorch.Leaky(beta=beta)\n",
    "        self.fc3 = torch.nn.Linear(num_hidden, 20)\n",
    "        self.lif3 = snntorch.Leaky(beta=beta, threshold=10e6)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.float()\n",
    "        x = x.permute(1,0,2)\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        V = []\n",
    "\n",
    "        # need to fix since data is not time leading axis...\n",
    "        for step in x:\n",
    "            cur1 = self.fc1(step)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            V.append(mem3)\n",
    "\n",
    "        \n",
    "        return torch.stack(V, axis=0).permute(1,0,2)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "acc = lambda predictions, targets : (torch.argmax(predictions, axis=-1) == targets).sum().item() / len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    \n",
    "    \n",
    "    train_batch = shuffle((x_train, y_train))\n",
    "    train_data, targets = train_batch\n",
    "    \n",
    "    \n",
    "    # Minibatch training loop\n",
    "    for data, targets in zip(train_data, targets):\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        out_V = net(data)\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = loss(torch.sum(out_V, axis=-2), targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Store loss history for future plotting\n",
    "    loss_hist.append(loss_val.item())\n",
    "\n",
    "\n",
    "# Test set\n",
    "with torch.no_grad():\n",
    "    denominator = y_test[0]\n",
    "    test_acc = 0\n",
    "    batch_acc = []\n",
    "    for test_data, test_targets in zip(x_test, y_test):\n",
    "        net.eval()\n",
    "        # Test set forward pass\n",
    "        out_V = net(test_data)\n",
    "        # Test set loss\n",
    "        batch_acc.append( acc(torch.sum(out_V, axis=-2), test_targets) )\n",
    "    \n",
    "    test_acc = np.mean(batch_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.016836166381836,\n",
       " 5.47111177444458,\n",
       " 4.38897180557251,\n",
       " 3.6949737071990967,\n",
       " 3.1036036014556885,\n",
       " 2.8180136680603027,\n",
       " 2.6146748065948486,\n",
       " 2.524266004562378,\n",
       " 2.364579200744629,\n",
       " 2.174179792404175,\n",
       " 1.7333829402923584,\n",
       " 1.7091920375823975,\n",
       " 1.74224853515625,\n",
       " 1.5204150676727295,\n",
       " 1.436014175415039,\n",
       " 1.3159263134002686,\n",
       " 1.1343700885772705,\n",
       " 1.1117918491363525,\n",
       " 1.13065505027771,\n",
       " 1.2796425819396973,\n",
       " 0.9392330050468445,\n",
       " 1.1473838090896606,\n",
       " 0.9924920797348022,\n",
       " 1.1947990655899048,\n",
       " 1.0341171026229858,\n",
       " 1.0296378135681152,\n",
       " 0.8375968337059021,\n",
       " 0.8479127287864685,\n",
       " 0.8093426823616028,\n",
       " 1.1181706190109253,\n",
       " 0.9630967378616333,\n",
       " 0.7824838757514954,\n",
       " 1.0526676177978516,\n",
       " 0.7316159605979919,\n",
       " 0.7704435586929321,\n",
       " 0.8092421293258667,\n",
       " 0.8359063863754272,\n",
       " 0.9393559098243713,\n",
       " 0.9334381818771362,\n",
       " 0.8651061058044434,\n",
       " 0.7047986388206482,\n",
       " 0.8912121653556824,\n",
       " 0.9294992685317993,\n",
       " 0.7106137275695801,\n",
       " 0.8002204895019531,\n",
       " 0.6694325804710388,\n",
       " 0.8686801195144653,\n",
       " 1.0326778888702393,\n",
       " 0.6218105554580688,\n",
       " 0.6384150981903076,\n",
       " 0.5991107821464539,\n",
       " 0.7732643485069275,\n",
       " 0.7393863797187805,\n",
       " 0.7650408148765564,\n",
       " 0.6782920360565186,\n",
       " 0.6336971521377563,\n",
       " 0.5887503623962402,\n",
       " 0.6236666440963745,\n",
       " 0.5987998247146606,\n",
       " 0.5077016949653625,\n",
       " 0.6958979368209839,\n",
       " 0.6925158500671387,\n",
       " 0.7544543743133545,\n",
       " 0.6833839416503906,\n",
       " 0.6740495562553406,\n",
       " 0.540213942527771,\n",
       " 0.5693359971046448,\n",
       " 0.514586865901947,\n",
       " 0.6732326745986938,\n",
       " 0.6379831433296204,\n",
       " 0.6642693281173706,\n",
       " 0.41494035720825195,\n",
       " 0.5590904951095581,\n",
       " 0.46755802631378174,\n",
       " 0.585815966129303,\n",
       " 0.4772145748138428,\n",
       " 0.5394749045372009,\n",
       " 0.6768158674240112,\n",
       " 0.6296069622039795,\n",
       " 0.44410526752471924,\n",
       " 0.5607107877731323,\n",
       " 0.6500124335289001,\n",
       " 0.6369630098342896,\n",
       " 0.5665066838264465,\n",
       " 0.4764293432235718,\n",
       " 0.44933995604515076,\n",
       " 0.48881101608276367,\n",
       " 0.4814888834953308,\n",
       " 0.5120143890380859,\n",
       " 0.4507831931114197,\n",
       " 0.7185863852500916,\n",
       " 0.5299605131149292,\n",
       " 0.5774674415588379,\n",
       " 0.5722068548202515,\n",
       " 0.5120803713798523,\n",
       " 0.5342373251914978,\n",
       " 0.4547029137611389,\n",
       " 0.4864477515220642,\n",
       " 0.46106740832328796,\n",
       " 0.5319391489028931,\n",
       " 0.5139029622077942,\n",
       " 0.5378866195678711,\n",
       " 0.521599292755127,\n",
       " 0.6224052309989929,\n",
       " 0.46496132016181946,\n",
       " 0.452244371175766,\n",
       " 0.4625670313835144,\n",
       " 0.4269678294658661,\n",
       " 0.47708702087402344,\n",
       " 0.43520277738571167,\n",
       " 0.5042893290519714,\n",
       " 0.45700907707214355,\n",
       " 0.4360080063343048,\n",
       " 0.49585047364234924,\n",
       " 0.4834817349910736,\n",
       " 0.41993677616119385,\n",
       " 0.5722962021827698,\n",
       " 0.39303693175315857,\n",
       " 0.33530643582344055,\n",
       " 0.4778425991535187,\n",
       " 0.3482072949409485,\n",
       " 0.3352729082107544,\n",
       " 0.46672695875167847,\n",
       " 0.39568275213241577,\n",
       " 0.4788668155670166,\n",
       " 0.5189787745475769,\n",
       " 0.3787730634212494,\n",
       " 0.4562118351459503,\n",
       " 0.2941484749317169,\n",
       " 0.40534836053848267,\n",
       " 0.3213733434677124,\n",
       " 0.35019946098327637,\n",
       " 0.5011698603630066,\n",
       " 0.36142727732658386,\n",
       " 0.43432706594467163,\n",
       " 0.5087982416152954,\n",
       " 0.47132742404937744,\n",
       " 0.3019081950187683,\n",
       " 0.49587854743003845,\n",
       " 0.3448406457901001,\n",
       " 0.33598053455352783,\n",
       " 0.3904065489768982,\n",
       " 0.38642773032188416,\n",
       " 0.3964524567127228,\n",
       " 0.35799598693847656,\n",
       " 0.3063018321990967,\n",
       " 0.33475834131240845,\n",
       " 0.3262307643890381,\n",
       " 0.4381583631038666,\n",
       " 0.37902164459228516,\n",
       " 0.4262310862541199,\n",
       " 0.3034527599811554,\n",
       " 0.3676610291004181,\n",
       " 0.45877552032470703,\n",
       " 0.34785979986190796,\n",
       " 0.24181164801120758,\n",
       " 0.4155271649360657,\n",
       " 0.2581157386302948,\n",
       " 0.3747130334377289,\n",
       " 0.4035416543483734,\n",
       " 0.3055797517299652,\n",
       " 0.4072335362434387,\n",
       " 0.43514493107795715,\n",
       " 0.4344475567340851,\n",
       " 0.44683679938316345,\n",
       " 0.3024548888206482,\n",
       " 0.30510634183883667,\n",
       " 0.4530971348285675,\n",
       " 0.22128300368785858,\n",
       " 0.34640687704086304,\n",
       " 0.26666006445884705,\n",
       " 0.233374685049057,\n",
       " 0.3017674684524536,\n",
       " 0.2856684923171997,\n",
       " 0.5009727478027344,\n",
       " 0.2669371962547302,\n",
       " 0.260023832321167,\n",
       " 0.25245681405067444,\n",
       " 0.37093666195869446,\n",
       " 0.33078497648239136,\n",
       " 0.3157902956008911,\n",
       " 0.29119813442230225,\n",
       " 0.4093058109283447,\n",
       " 0.2807322144508362,\n",
       " 0.2516176402568817,\n",
       " 0.2827915549278259,\n",
       " 0.24319441616535187,\n",
       " 0.24468323588371277,\n",
       " 0.19468261301517487,\n",
       " 0.2567092776298523,\n",
       " 0.3506395220756531,\n",
       " 0.3395957350730896,\n",
       " 0.3143686056137085,\n",
       " 0.21695448458194733,\n",
       " 0.29574573040008545,\n",
       " 0.3443997800350189,\n",
       " 0.1664290577173233,\n",
       " 0.30659639835357666,\n",
       " 0.2745823264122009,\n",
       " 0.22450602054595947,\n",
       " 0.3364424407482147,\n",
       " 0.19567599892616272,\n",
       " 0.22932128608226776,\n",
       " 0.28728827834129333,\n",
       " 0.22472351789474487,\n",
       " 0.2339084893465042,\n",
       " 0.25783270597457886,\n",
       " 0.19267389178276062,\n",
       " 0.2624070644378662,\n",
       " 0.2528569996356964,\n",
       " 0.3090193271636963,\n",
       " 0.2121441662311554,\n",
       " 0.3122534453868866,\n",
       " 0.3186034560203552,\n",
       " 0.3965374529361725,\n",
       " 0.2767317295074463,\n",
       " 0.22399677336215973,\n",
       " 0.20794832706451416,\n",
       " 0.2414422184228897,\n",
       " 0.2742871940135956,\n",
       " 0.1763298213481903,\n",
       " 0.19243276119232178,\n",
       " 0.2635750472545624,\n",
       " 0.14489592611789703,\n",
       " 0.16223381459712982,\n",
       " 0.16964244842529297,\n",
       " 0.19086509943008423,\n",
       " 0.19184286892414093,\n",
       " 0.201761394739151,\n",
       " 0.18815214931964874,\n",
       " 0.179782435297966,\n",
       " 0.14407432079315186,\n",
       " 0.15168827772140503,\n",
       " 0.20914502441883087,\n",
       " 0.21298885345458984,\n",
       " 0.18412475287914276,\n",
       " 0.18697941303253174,\n",
       " 0.24546638131141663,\n",
       " 0.18889646232128143,\n",
       " 0.22749993205070496,\n",
       " 0.174546480178833,\n",
       " 0.23616933822631836,\n",
       " 0.24136856198310852,\n",
       " 0.2137475460767746,\n",
       " 0.23303601145744324,\n",
       " 0.21770919859409332,\n",
       " 0.253343403339386,\n",
       " 0.1144520565867424,\n",
       " 0.1876479983329773,\n",
       " 0.217819482088089,\n",
       " 0.18045859038829803,\n",
       " 0.16099542379379272,\n",
       " 0.22568438947200775,\n",
       " 0.18257354199886322,\n",
       " 0.2111380398273468,\n",
       " 0.27738046646118164,\n",
       " 0.1891316920518875,\n",
       " 0.12918154895305634,\n",
       " 0.24918876588344574,\n",
       " 0.10136639326810837,\n",
       " 0.26933062076568604,\n",
       " 0.19194132089614868,\n",
       " 0.16285833716392517,\n",
       " 0.19146700203418732,\n",
       " 0.23743583261966705,\n",
       " 0.12389949709177017,\n",
       " 0.18164849281311035,\n",
       " 0.17158645391464233,\n",
       " 0.1292787790298462,\n",
       " 0.11633958667516708,\n",
       " 0.20882126688957214,\n",
       " 0.11696721613407135,\n",
       " 0.16695468127727509,\n",
       " 0.15920524299144745,\n",
       " 0.1150883361697197,\n",
       " 0.13660311698913574,\n",
       " 0.12815643846988678,\n",
       " 0.15192902088165283,\n",
       " 0.17920532822608948,\n",
       " 0.10319172590970993,\n",
       " 0.15106654167175293,\n",
       " 0.10285717248916626,\n",
       " 0.1536804884672165,\n",
       " 0.11059670895338058,\n",
       " 0.14225773513317108,\n",
       " 0.1310524046421051,\n",
       " 0.10881145298480988,\n",
       " 0.11264654994010925,\n",
       " 0.20357517898082733,\n",
       " 0.21456919610500336,\n",
       " 0.12273383885622025,\n",
       " 0.1017044335603714,\n",
       " 0.16568957269191742,\n",
       " 0.17671510577201843,\n",
       " 0.1915992647409439,\n",
       " 0.15443675220012665,\n",
       " 0.15046709775924683,\n",
       " 0.18687283992767334,\n",
       " 0.1445682942867279,\n",
       " 0.11914792656898499]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66015625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
