{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6793a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".80\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import spyx\n",
    "import haiku as hk\n",
    "import optax\n",
    "from jax_tqdm import scan_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30d5e6",
   "metadata": {},
   "source": [
    "### SHD Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab70fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tonic\n",
    "from tonic import datasets, transforms\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple(\"State\", \"obs labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d34a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SHD2Raster():\n",
    "    \"\"\" \n",
    "    Tool for rastering SHD samples into frames. Packs bits along the temporal axis for memory efficiency. This means\n",
    "        that the used will have to apply jnp.unpackbits(events, axis=<time axis>) prior to feeding the data to the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_dim, sample_T = 100):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.sample_T = sample_T\n",
    "        \n",
    "    def __call__(self, events):\n",
    "        # tensor has dimensions (time_steps, encoding_dim)\n",
    "        tensor = np.zeros((events[\"t\"].max()+1, self.encoding_dim), dtype=int)\n",
    "        np.add.at(tensor, (events[\"t\"], events[\"x\"]), 1)\n",
    "        #return tensor[:self.sample_T,:]\n",
    "        tensor = tensor[:self.sample_T,:]\n",
    "        tensor = np.minimum(tensor, 1)\n",
    "        tensor = np.packbits(tensor, axis=0)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a2e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_T = 64\n",
    "shd_timestep = 1e-6\n",
    "shd_channels = 700\n",
    "net_channels = 128\n",
    "net_dt = 1/sample_T\n",
    "batch_size = 256\n",
    "\n",
    "obs_shape = tuple([net_channels,])\n",
    "act_shape = tuple([20,])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Downsample(\n",
    "        time_factor=shd_timestep / net_dt,\n",
    "        spatial_factor=net_channels / shd_channels\n",
    "    ),\n",
    "    _SHD2Raster(net_channels, sample_T=sample_T)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.SHD(\"./data\", train=True, transform=transform)\n",
    "test_dataset = datasets.SHD(\"./data\", train=False, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf6be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(DataLoader(train_dataset, batch_size=len(train_dataset),\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=True), drop_last=True, shuffle=False))\n",
    "        \n",
    "x_train, y_train = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2544a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = iter(DataLoader(test_dataset, batch_size=len(test_dataset),\n",
    "                          collate_fn=tonic.collation.PadTensors(batch_first=True), drop_last=True, shuffle=False))\n",
    "        \n",
    "x_test, y_test = next(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69c12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([8156, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670ea0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = jnp.array(x_train, dtype=jnp.uint8)\n",
    "y_train = jnp.array(y_train, dtype=jnp.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7135a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(dataset, shuffle_rng):\n",
    "    x, y = dataset\n",
    "\n",
    "    cutoff = y.shape[0] % batch_size\n",
    "\n",
    "    obs = jax.random.permutation(shuffle_rng, x, axis=0)[:-cutoff]\n",
    "    labels = jax.random.permutation(shuffle_rng, y, axis=0)[:-cutoff]\n",
    "\n",
    "    obs = jnp.reshape(obs, (-1, batch_size) + obs.shape[1:])\n",
    "    labels = jnp.reshape(labels, (-1, batch_size)) # should make batch size a global\n",
    "\n",
    "    return State(obs=obs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d42116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "x, y = x_train, y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f415eb8",
   "metadata": {},
   "source": [
    "### Spyx SHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddc797e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shd_snn(x): # need to constrain beta to be 0.8...\n",
    "        \n",
    "    core = hk.DeepRNN([\n",
    "        hk.Linear(64, with_bias=False),\n",
    "        spyx.nn.LIF((64,), beta=0.8, activation=spyx.axn.Axon(spyx.axn.arctan())),\n",
    "        hk.Linear(64, with_bias=False),\n",
    "        spyx.nn.LIF((64,), beta=0.8, activation=spyx.axn.Axon(spyx.axn.arctan())),\n",
    "        hk.Linear(20, with_bias=False),\n",
    "        spyx.nn.LI((20,), beta=0.8)\n",
    "    ])\n",
    "    \n",
    "    # static unroll for maximum performance\n",
    "    spikes, V = hk.dynamic_unroll(core, x, core.initial_state(x.shape[0]), time_major=False, unroll=32)\n",
    "    \n",
    "    return spikes, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344ed98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "# Since there's nothing stochastic about the network, we can avoid using an RNG as a param!\n",
    "sample_x, sample_y = shuffle((x,y),key)\n",
    "SNN = hk.without_apply_rng(hk.transform(shd_snn))\n",
    "params = SNN.init(rng=key, x=jnp.float16(sample_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f7422a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(SNN, params, dataset, epochs=300):\n",
    "        \n",
    "    opt = optax.adam(learning_rate=5e-4)\n",
    "    \n",
    "    # create and initialize the optimizer\n",
    "    opt_state = opt.init(params)\n",
    "    grad_params = params\n",
    "        \n",
    "    # define and compile our eval function that computes the loss for our SNN\n",
    "    @jax.jit\n",
    "    def net_eval(weights, events, targets):\n",
    "        readout = SNN.apply(weights, events)\n",
    "        traces, V_f = readout\n",
    "        return spyx.fn.integral_crossentropy(traces, targets) # smoothing needs to be more explicit in docs...\n",
    "        \n",
    "    # Use JAX to create a function that calculates the loss and the gradient!\n",
    "    surrogate_grad = jax.value_and_grad(net_eval) \n",
    "        \n",
    "    rng = jax.random.PRNGKey(0)        \n",
    "    \n",
    "    # compile the meat of our training loop for speed\n",
    "    @jax.jit\n",
    "    def train_step(state, data):\n",
    "        grad_params, opt_state = state\n",
    "        events, targets = data # fix this\n",
    "        events = jnp.unpackbits(events, axis=1) # decompress temporal axis\n",
    "        # compute loss and gradient                    # need better augment rng\n",
    "        loss, grads = surrogate_grad(grad_params, events, targets)\n",
    "        # generate updates based on the gradients and optimizer\n",
    "        updates, opt_state = opt.update(grads, opt_state, grad_params)\n",
    "        # return the updated parameters\n",
    "        new_state = [optax.apply_updates(grad_params, updates), opt_state]\n",
    "        return new_state, loss\n",
    "    \n",
    "    \n",
    "    # Here's the start of our training loop!\n",
    "    @scan_tqdm(epochs)\n",
    "    def epoch(epoch_state, epoch_num):\n",
    "        curr_params, curr_opt_state = epoch_state\n",
    "\n",
    "        shuffle_rng = jax.random.fold_in(rng, epoch_num)\n",
    "        train_data = shuffle(dataset, shuffle_rng)\n",
    "        \n",
    "        # train epoch\n",
    "        end_state, train_loss = jax.lax.scan(\n",
    "            train_step,# func\n",
    "            [curr_params, curr_opt_state],# init\n",
    "            train_data,# xs\n",
    "            train_data.obs.shape[0]# len\n",
    "        )\n",
    "                    \n",
    "        return end_state, jnp.mean(train_loss)\n",
    "    # end epoch\n",
    "    \n",
    "    # epoch loop\n",
    "    final_state, metrics = jax.lax.scan(\n",
    "        epoch,\n",
    "        [grad_params, opt_state], # metric arrays\n",
    "        jnp.arange(epochs), # \n",
    "        epochs # len of loop\n",
    "    )\n",
    "    \n",
    "    final_params, _ = final_state\n",
    "    \n",
    "                \n",
    "    # return our final, optimized network.       \n",
    "    return final_params, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a5cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gd(SNN, params, dataset):\n",
    "\n",
    "    @jax.jit\n",
    "    def test_step(params, data):\n",
    "        events, targets = data\n",
    "        events = jnp.unpackbits(events, axis=1)\n",
    "        readout = SNN.apply(params, events)\n",
    "        traces, V_f = readout\n",
    "        acc, pred = spyx.fn.integral_accuracy(traces, targets)\n",
    "        loss = spyx.fn.integral_crossentropy(traces, targets)\n",
    "        return params, [acc, loss, pred, targets]\n",
    "    \n",
    "    test_data = shuffle(dataset, jax.random.PRNGKey(0))\n",
    "    \n",
    "    _, test_metrics = jax.lax.scan(\n",
    "            test_step,# func\n",
    "            params,# init\n",
    "            test_data,# xs\n",
    "            test_data.obs.shape[0]# len\n",
    "    )\n",
    "    \n",
    "    acc = jnp.mean(test_metrics[0])\n",
    "    loss = jnp.mean(test_metrics[1])\n",
    "    preds = jnp.array(test_metrics[2]).flatten()\n",
    "    tgts = jnp.array(test_metrics[3]).flatten()\n",
    "    return acc, loss, preds, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd08a737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c43a8438004107bb012532453dd133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_params, metrics = gd(SNN, params, (x,y), epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7a3a0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([9.956848 , 3.3741896, 3.0981524, 3.098117 , 3.10788  , 3.0800176,\n",
       "       3.044519 , 3.0078552, 2.9578753, 2.9254699, 2.9000227, 2.855232 ,\n",
       "       2.825492 , 2.8034348, 2.7728598, 2.7524285, 2.7449524, 2.7288525,\n",
       "       2.6960046, 2.6906924, 2.6746328, 2.656041 , 2.648154 , 2.628879 ,\n",
       "       2.6072752, 2.59138  , 2.5787544, 2.55287  , 2.537342 , 2.5289085,\n",
       "       2.5115821, 2.492704 , 2.485445 , 2.4841418, 2.4817295, 2.461743 ,\n",
       "       2.4588144, 2.4411714, 2.4179587, 2.4100943, 2.4141183, 2.4042754,\n",
       "       2.3774354, 2.371575 , 2.3634982, 2.3623655, 2.3439069, 2.3433695,\n",
       "       2.3426375, 2.3337328, 2.326334 , 2.3270352, 2.3221889, 2.3201895,\n",
       "       2.31026  , 2.3067636, 2.2986856, 2.2941604, 2.2861533, 2.2901187,\n",
       "       2.2763484, 2.2621   , 2.2566044, 2.2695832, 2.2541065, 2.2461262,\n",
       "       2.249047 , 2.245604 , 2.2367456, 2.2299867, 2.2237453, 2.2228508,\n",
       "       2.2209733, 2.2107854, 2.2181134, 2.2184722, 2.2136185, 2.204432 ,\n",
       "       2.1955223, 2.207146 , 2.200374 , 2.1876988, 2.189381 , 2.184691 ,\n",
       "       2.1792293, 2.1802115, 2.1802914, 2.1762393, 2.1745243, 2.170153 ,\n",
       "       2.1599114, 2.163507 , 2.1658058, 2.1613247, 2.152515 , 2.1571746,\n",
       "       2.1585653, 2.1548758, 2.1530688, 2.150765 , 2.149445 , 2.1582956,\n",
       "       2.1370473, 2.1315908, 2.1310587, 2.1351678, 2.120041 , 2.1227808,\n",
       "       2.1259377, 2.1209867, 2.1198595, 2.1123948, 2.1174889, 2.1180058,\n",
       "       2.116225 , 2.1166666, 2.1127002, 2.1095662, 2.1173825, 2.1161616,\n",
       "       2.110582 , 2.1029296, 2.103371 , 2.0992877, 2.0867167, 2.0960398,\n",
       "       2.088378 , 2.0933423, 2.090172 , 2.08971  , 2.0826395, 2.0880756,\n",
       "       2.0779974, 2.077372 , 2.0746503, 2.0765052, 2.0745804, 2.0729744,\n",
       "       2.078476 , 2.0780592, 2.0742786, 2.0781555, 2.0681212, 2.06057  ,\n",
       "       2.0745482, 2.0705254, 2.0700235, 2.06251  , 2.0614748, 2.053375 ,\n",
       "       2.0629215, 2.0567465, 2.0542548, 2.053532 , 2.0534813, 2.0479078,\n",
       "       2.0505466, 2.0543377, 2.0520775, 2.0558295, 2.0529222, 2.0504372,\n",
       "       2.0470731, 2.0476894, 2.0447218, 2.035904 , 2.0481458, 2.0307257,\n",
       "       2.0316386, 2.0376444, 2.0326908, 2.0294268, 2.0350728, 2.0266752,\n",
       "       2.0269008, 2.019479 , 2.026702 , 2.0211482, 2.0281928, 2.0180297,\n",
       "       2.0210056, 2.0139866, 2.011802 , 2.017066 , 2.012885 , 2.0131857,\n",
       "       2.0130992, 2.01025  , 2.0138524, 2.0037453, 2.0141094, 2.018734 ,\n",
       "       2.0068064, 2.0057702, 2.009953 , 1.9979956, 1.999803 , 2.0021768,\n",
       "       2.002523 , 2.004409 , 2.0020041, 1.9978129, 1.9938685, 1.9953266,\n",
       "       1.9963937, 1.989389 , 1.992143 , 1.9926697, 1.9927833, 1.9855725,\n",
       "       1.9932673, 1.9898196, 1.9858551, 1.9882526, 1.9855053, 1.9810234,\n",
       "       1.9891216, 1.9807206, 1.9799237, 1.9840311, 1.9785825, 1.983596 ,\n",
       "       1.9898231, 1.9866698, 1.988179 , 1.9740106, 1.9812859, 1.9855016,\n",
       "       1.9807719, 1.9751943, 1.9805497, 1.9784813, 1.9780145, 1.977088 ,\n",
       "       1.9803168, 1.9686714, 1.975896 , 1.9820962, 1.9751233, 1.9748509,\n",
       "       1.9712217, 1.9676715, 1.9642283, 1.9732279, 1.9661359, 1.9634243,\n",
       "       1.9618195, 1.963371 , 1.9604486, 1.9600792, 1.9652026, 1.9601958,\n",
       "       1.9611979, 1.9657001, 1.972875 , 1.9585011, 1.9594584, 1.9605769,\n",
       "       1.9610616, 1.9611136, 1.9603899, 1.9630079, 1.9677134, 1.9553032,\n",
       "       1.9660759, 1.9509115, 1.9547871, 1.9500606, 1.9581988, 1.9500723,\n",
       "       1.9469955, 1.9499502, 1.9431286, 1.9479113, 1.943736 , 1.9446244,\n",
       "       1.9413692, 1.9490678, 1.946494 , 1.9388016, 1.9413104, 1.9403361,\n",
       "       1.9475687, 1.9458098, 1.9376404, 1.9474081, 1.9410615, 1.9352722,\n",
       "       1.9352196, 1.9362429, 1.9425223, 1.9366636, 1.9316378, 1.9333333,\n",
       "       1.9302689, 1.9307388, 1.9351221, 1.9312747, 1.9283164, 1.9354968],      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8553879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81413805 Loss: 1.9373717\n"
     ]
    }
   ],
   "source": [
    "acc, loss, preds, tgts = test_gd(SNN, grad_params, (x,y))\n",
    "print(\"Accuracy:\", acc, \"Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
